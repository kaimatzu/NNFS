{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Activation Function\n",
    "\n",
    "The previous activation function, the rectified linear unit, lacks normalization and exclusivity. This means its outputs are unbounded, contextually unclear, and independent of each other. To overcome this, the softmax activation is introduced. It takes uncalibrated inputs and transforms them into a normalized probability distribution, providing context and making the outputs mutually exclusive.\n",
    "\n",
    "This is especially important in classification. In the case of classification, what we want to see is a prediction of which class the network “thinks” the input represents. This distribution returned by the softmax activation function represents **confidence scores** for each class and will add up to 1. The predicted class is associated with the output neuron that returned the largest confidence score.\n",
    "\n",
    "Here’s the formula for the Softmax Activation Function:\n",
    "\n",
    "$$ \n",
    "S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Here's the breakdown of the components:\n",
    "\n",
    "- $ S_{i,j} $: This represents the output of the Softmax function for the $i$-th sample and the $j$-th class. It gives the probability that the $i$-th sample belongs to the $j$-th class.\n",
    "\n",
    "- $ e^{z_{i,j}} $: This is the exponential of the input logits $z_{i,j}$ for the $i$-th sample and the $j$-th class. Exponentiating the logits ensures that the values are non-negative. (Simply, this means $e$ raised to the power of the $i$-th input of the $j$-th class).\n",
    "\n",
    "- $ \\sum_{l=1}^{L} e^{z_{i,l}} $: This is the sum of the exponentiated logits over all classes ($L$ is the total number of classes). It represents the normalization term to ensure that the probabilities sum to 1.\n",
    "\n",
    "The Softmax function essentially calculates the exponential of each input score, normalizes these values by dividing by the sum of all exponentiated scores, and gives the final probabilities for each class. This is crucial in classification tasks, where the goal is to assign a probability distribution over multiple classes for each input.\n",
    "\n",
    "In python, it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate the exponential value \n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values) \n",
    "print('normalized exponentiated values:')\n",
    "print (norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
